{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98fb8e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Open the .jsonl file and read its content\n",
    "with open('./Qwen2.5-Math/evaluation/data/aime24/test.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse each line as a JSON object\n",
    "        json_data = json.loads(line)\n",
    "        \n",
    "        # Append the parsed JSON data (dictionary) to the data list\n",
    "        data.append(json_data)\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783199c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_32bit=True)  # Or load_in_4bit=True if needed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-Math-7B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\",\n",
    "    quantization_config=quantization_config  # Apply the new config\n",
    ")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Math-7B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233bd1a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "true_answer = 0\n",
    "for i, row in df.iterrows():\n",
    "    prompt = (\n",
    "        \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{row['problem']}\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    answer = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    if f'boxed{row[\"answer\"]}' in answer:\n",
    "        true_answer += 1\n",
    "    print(f'\\nSample: {i} --- Answer: {answer[-100:]}')\n",
    "    print(f'\\nSample: {i} --- Number correct answer: {true_answer}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d106f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "true_answer = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    problem_text = df.iloc[i]['problem']\n",
    "\n",
    "    prompt = (\n",
    "        \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{problem_text}\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Schedule for temperature\n",
    "    def temperature_schedule(step):\n",
    "        if step < 10:\n",
    "            return 0.8\n",
    "        elif step < 200:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.2\n",
    "\n",
    "    # Generation setup\n",
    "    generated = input_ids\n",
    "    answer_tokens = []\n",
    "    past_key_values = None\n",
    "    max_new_tokens = 1024\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            if step == 0:\n",
    "                outputs = model(input_ids=generated, use_cache=True)\n",
    "            else:\n",
    "                outputs = model(input_ids=next_token, use_cache=True, past_key_values=past_key_values)\n",
    "\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            temp = temperature_schedule(step)\n",
    "            probs = F.softmax(logits / temp, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        generated = torch.cat([generated, next_token], dim=-1)\n",
    "        answer_tokens.append(next_token[0].item())\n",
    "\n",
    "        if next_token[0].item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Decode only once at the end\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "    print(f'\\nSample: {i} --- Answer: {answer[-100:]}')\n",
    "    if f'\\\\boxed{{{df.iloc[i][\"answer\"]}}}' in answer:\n",
    "        true_answer += 1\n",
    "\n",
    "    print(f'Sample: {i} --- Number correct answer: {true_answer}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
